# NLP-cs224n_lab1-5
CS224N-Stanford-Winter-2021
The collection of ALL relevant materials about CS224N-Stanford/Winter 2021 course. THANKS TO THE PROFESSOR AND TAs!
All the rights of the relevant materials belong to Standfor University.
斯坦福大学CS224N 【2021】课程的【所有】相关的资料。感谢Chris Manning教授和Abigail See，感谢所有助教！
课程所有资料所有权属于斯坦福大学。

__课程主页__：[Stanford / Winter 2021](http://web.stanford.edu/class/cs224n/)

__课程视频__:[YouTube](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=1)

__国内视频资源__：[【斯坦福CS224N】(2021|中英) 深度自然语言处理Natural Language Processing with Deep Learning](https://www.bilibili.com/video/BV18Y411p79k/?spm_id_from=333.337.search-card.all.click&vd_source=383492af990fc99e468d8084dc0e2e30)


__Assignments__ <br />
[lab1:](https://github.com/eeeedy/NLP-cs224n_lab1-5/tree/main/lab1): Exploring Word Vectors ✅<br />
[lab2:](https://github.com/eeeedy/NLP-cs224n_lab1-5/tree/main/lab2)word2vec ✅<br />
[lab3:](https://github.com/eeeedy/NLP-cs224n_lab1-5/tree/main/lab3(Neural Dependency Parsing)) Neural Dependency Parsing ✅ <br />
[lab4:](https://github.com/eeeedy/NLP-cs224n_lab1-5/tree/main/lab4(Seq2Seq Model with Attention)) Neural Machine Translation with RNNs and Analyzing NMT Systems ✅ <br />
[lab5:](https://github.com/eeeedy/NLP-cs224n_lab1-5/tree/main/lab5(Pretraining and Fine_tuning Transformer Model))Self-Attention, Transformers, and Pretraining ✅ <br />


Usage:
In the environment of git installed, use the command as follows:
__$git clone https://github.com/eeeedy/NLP-cs224n_lab1-5__
And you will get all the materials of this excellent course.
Or, just click the green button "Clone or download" of this repo, then choose the "Download Zip", and you will get them.

使用方法：
在git已安装的环境中，使用以下命令：
__$git clone https://github.com/eeeedy/NLP-cs224n_lab1-5__
你就会得到这门课程的所有的相关资料。
或者，点击该仓库中绿色“Clone or download”按钮，再选择“Download Zip”，也会得到同样效果。
